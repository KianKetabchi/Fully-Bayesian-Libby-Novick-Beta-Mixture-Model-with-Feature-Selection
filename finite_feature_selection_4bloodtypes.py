# -*- coding: utf-8 -*-
"""Finite_feature_selection_4bloodtypes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VTBjCjReS0jjobRgf9qUHt9nk4SgEuMU
"""

import numpy as np

import pandas as pd

import math

import scipy

from scipy.stats import beta,bernoulli

from scipy.special import gammaln

from scipy.special import gamma

from scipy.stats import dirichlet

from numpy import asarray

import sys

from sklearn import datasets

from sklearn import preprocessing

from sklearn.cluster import KMeans

from numpy import min as MIN, max as MAX

from numpy import mean as MEAN

from numpy import var as VAR

import matplotlib.pyplot as plt

from sklearn.metrics import confusion_matrix 

from sklearn.metrics import accuracy_score 

from sklearn.metrics import classification_report 

from sklearn.metrics import accuracy_score

from sklearn.metrics import precision_score

from sklearn.metrics import recall_score

from sklearn.metrics import f1_score

from sklearn.metrics import precision_recall_fscore_support

import pylab as pl

from scipy.stats import dirichlet

dataset = pd.read_csv('Blood Type.csv')


data_x = dataset.iloc[:, 0:dataset.shape[1]-1]

y = dataset.iloc[:,-1].values



num_sample = data_x.shape[0]
num_sample
#num_dimension = data_x.shape[1]
#num_dimension

num_dimension = data_x.shape[1]
num_dimension

#normalizing data

min_max_scaler = preprocessing.MinMaxScaler()

data_normalize = min_max_scaler.fit_transform(data_x)

for i in range(num_sample):

    for j in range(num_dimension):

        if data_normalize[i,j] ==0: 

            data_normalize[i,j]= 0.00001

        elif data_normalize[i,j]==1:

            data_normalize[i,j]= 0.9999

x = data_normalize

for k in range(num_dimension):
  x_k = x[:,k]

from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters = 4)
kmeans.fit(x)
class_labels = kmeans.labels_

confusion_matrix(class_labels,y)

def split_pixels_based_on_label(labels, data):

    clusters_obj = {}

    for index, label in enumerate(labels):

        if not (label in clusters_obj):

            clusters_obj[label] = []

        clusters_obj[label].append(data[index])

    return clusters_obj





clusters_set= split_pixels_based_on_label(class_labels, x)

clusters_set

#Determine LN Parameters
num_cluster= 4

a =  np.array([1, 2, 6, 1, 4, 9, 9, 9, 1, 6,1, 6, 6, 9, 4, 9, 1, 2, 1, 1,9, 9, 1, 2, 9, 6, 3, 6, 8, 5,9, 6, 3,8, 9, 4, 1, 9, 1, 2]).reshape(num_cluster,num_dimension)

b = np.array([5, 1, 1, 9, 4, 1, 9, 3, 8, 9, 9, 9, 9, 9, 3, 2, 7, 6, 6, 9,1 ,5 ,9 ,1 ,1 ,6 ,1 ,9 ,1 ,5,8, 9, 2, 1, 1, 1, 5, 7, 9, 8]).reshape(num_cluster,num_dimension)

c = np.ones([num_cluster,num_dimension]) 


#a_irr = np.random.randint(1,10,(num_cluster,num_dimension))
a_irr= np.sum(a,axis =0).reshape(num_dimension,1)
#b_irr = np.random.randint(1,10,(num_cluster,num_dimension))
b_irr= np.sum(b,axis =0).reshape(num_dimension,1)
#c_irr = np.ones([num_cluster,num_dimension]) 
c_irr= np.sum(c,axis =0).reshape(num_dimension,1)

a_irr.shape

theta_matrix_initial = np.array([a,b,c]).reshape(3,num_dimension*num_cluster)
theta_matrix_initial

theta_matrix_irr_initial = np.array([a_irr,b_irr,c_irr]).reshape(3,num_dimension)
theta_matrix_irr_initial

def mixer_estimator(cluster_set, p_size):

    return [len(cluster_set[cluster])/p_size for cluster in cluster_set]



mix = mixer_estimator(clusters_set, num_sample)

mix

for j in range(num_cluster):
  print(a[j])
  print(b[j])
  print(c[j])

def LN_pdf(a,b,c,x):
  second_part_one = np.matmul(np.log(x),(a-1).T)
  one_minus_data = np.subtract(1 , x) + 0.001
  third_part_one = np.matmul(np.log(one_minus_data),(b-1).T)   
  fifth_part = 1
  pdf = second_part_one+third_part_one+fifth_part
  return pdf

ln_pdf = LN_pdf(a,b,c,x)
ln_pdf

def LN_pdf_irr(a_irr,b_irr,c_irr,x):
  dd = np.log(x[:,k])
  ddd = dd.reshape(num_sample,1)
  second_part_one = np.matmul(ddd,(a_irr-1).T)

  one_minus_data = np.subtract(1 , x[:,k]) + 0.001
  one_minus_data = one_minus_data.reshape(num_sample,1)
  third_part_one = np.matmul(np.log(one_minus_data),(b_irr-1).T)   
  fifth_part = 1
  pdf = second_part_one+third_part_one+fifth_part
  return pdf

ln_pdf_irr = LN_pdf_irr(a_irr,b_irr,c_irr,x)
ln_pdf_irr

rho = np.ones([num_cluster,num_dimension])*0.95

for j in range(num_cluster):
  for k in range(num_dimension):
    print(rho[j,k])

def LN_pdf_feature_selction(x, rho, a, b, c, a_irr, b_irr, c_irr):
  
    # y = mpmath.mpf(1.0)
    for k in range(num_dimension):
      e1 = rho[j,k] * ln_pdf
      w = beta.pdf(x[:,k],a_irr,1/b_irr)
      e2= np.matmul(1-rho, w).T
      y = e1+e2
      posteriori = (y*mix). reshape(num_sample , num_cluster)
      zhat = posteriori/np.asarray(np.sum(posteriori, axis=1)).reshape(num_sample, 1)
    return zhat

zhat = LN_pdf_feature_selction(x, rho, a, b, c, a_irr, b_irr, c_irr)
zhat

#exp(log)
def zed(zhat):
    z = np.zeros(zhat.shape)
    z[np.arange(len(zhat)), zhat.argmax(1)] = 1
    return (z)

z_matrix = zed(zhat)
z_matrix

df = pd.DataFrame(zhat)
df.plot.hist()
plt.show()

def z_creator (z_matrix):

    z_label = np.zeros((num_sample,1))

    for i in range(num_sample):

        for j in range(num_cluster):

            if z_matrix[i,j]== 1:

                z_label[i] = j

    return z_label

    

z_label = z_creator(z_matrix)
z_label

z_matrix

def nt(z):
  nt = np.zeros(z.shape)
  nt = np.sum(z,axis = 0)
  return(nt)

n_counter = nt(z_matrix)
n_counter

# posterior distribution for weight (Equation)
eta = ([1/num_dimension]*num_cluster)
def probability(eta,n_counter):
  p = np.random.dirichlet(eta+n_counter)
  return p

#def transition_model(num_cluster, theta_matrix_initial):

    #sigma = np.ones((theta_matrix_initial.shape[1])) * 0.15

    #return np.random.normal(theta_matrix_initial,list(sigma))

#tr = transition_model(num_cluster, theta_matrix_initial)
#tr

#hyper_prameter Matrix
u = np.random.randint(1,10,(num_cluster,num_dimension))
v = np.random.randint(1,10,(num_cluster,num_dimension))
r = np.random.randint(1,10,(num_cluster,num_dimension))
s = np.random.randint(1,10,(num_cluster,num_dimension))
f = np.random.randint(1,10,(num_cluster,num_dimension))
g = np.random.randint(1,10,(num_cluster,num_dimension))

u_irr = np.sum(u,axis =0).reshape(num_dimension,1)
v_irr = np.sum(v,axis =0).reshape(num_dimension,1)
r_irr = np.sum(r,axis =0).reshape(num_dimension,1)
s_irr = np.sum(s,axis =0).reshape(num_dimension,1)
f_irr = np.sum(f,axis =0).reshape(num_dimension,1)
g_irr = np.sum(g,axis =0).reshape(num_dimension,1)

# conditional posterior for (a,b,c) and (a^,b^,c^)

# check the sum 
def log_prior1(a,b,c):
  log_p_c = np.absolute((f*np.log(g))+((f-1)*np.log(c))-(g*c)-(gammaln(f)))
  log_p_a = np.absolute((u*np.log(v))+((u-1)*np.log(a))-(v*a)-(gammaln(u)))
  log_p_b = np.absolute((r*np.log(s))+((r-1)*np.log(b))-(s*b)-(gammaln(r)))
  return log_p_a,log_p_b,log_p_c

log_c,log_b,log_a= log_prior1(a,b,c)

# a,b,c posteriors
posterior = log_c*log_b*log_a
posterior

def log_prior_irr(a_irr,b_irr,c_irr):
  log_p_c_irr = np.absolute((f_irr*np.log(g_irr))+((f_irr-1)*np.log(c_irr))-(g_irr*c_irr)-(gammaln(f_irr)))
  log_p_a_irr = np.absolute((u_irr*np.log(v_irr))+((u_irr-1)*np.log(a_irr))-(v_irr*a_irr)-(gammaln(u_irr)))
  log_p_b_irr = np.absolute((r_irr*np.log(s_irr))+((r_irr-1)*np.log(b_irr))-(s_irr*b_irr)-(gammaln(r_irr)))
  return log_p_a_irr,log_p_b_irr,log_p_c_irr

log_c_irr,log_b_irr,log_a_irr= log_prior_irr(a_irr,b_irr,c_irr)

# a,b,c posteriors
posterior_irr = log_c_irr*log_b_irr*log_a_irr
posterior_irr

a_new= np.random.lognormal(a,0.5,(num_cluster,num_dimension))
a_new = np.array(a_new)
b_new = np.random.lognormal(b,0.55,(num_cluster,num_dimension))
b_new = np.array(b_new)
c_new = np.random.lognormal(c,0.45,(num_cluster,num_dimension))
c_new = np.array(c_new)



a_irr_new= np.random.lognormal(a_irr,0.5,(num_dimension,1))
a_irr_new = np.array(a_irr_new)
b_irr_new = np.random.lognormal(b_irr,0.55,(num_dimension,1))
b_irr_new = np.array(b_irr_new)
c_irr_new = np.random.lognormal(c_irr,0.45,(num_dimension,1))
c_irr_new = np.array(c_irr_new)


theta_matrix_new = np.array([a_new,b_new,c_new]).reshape(3,num_dimension*num_cluster)
theta_matrix_new

theta_matrix_new_irr = np.array([a_irr_new,b_irr_new,c_irr_new]).reshape(3,num_dimension)
theta_matrix_new_irr

ln_pdf_new = LN_pdf(a_new,b_new,c_new,x)
ln_pdf_new

ln_pdf_irr_new = LN_pdf_irr(a_irr_new,b_irr_new,c_irr_new,x)
ln_pdf_irr_new

log_prior_new = log_prior1(a_new,b_new,c_new)
log_prior_new

log_prior_old = log_prior1(a,b,c)
log_prior_old

log_prior_new_irr = log_prior_irr(a_irr_new,b_irr_new,c_irr_new)
log_prior_new_irr

log_prior_old_irr = log_prior_irr(a_irr,b_irr,c_irr)
log_prior_old_irr

#theta
theta_initial= np.sum(theta_matrix_initial,axis = 1)
theta_initial = np.sum(theta_initial,axis = 0)
#theta_initial

theta_new = np.sum(theta_matrix_new,axis =1)
theta_new = np.sum(theta_new,axis = 0)
#theta_new

#theta irr
theta_initial_irr= np.sum(theta_matrix_irr_initial,axis = 1)
theta_initial_irr = np.sum(theta_initial,axis = 0)
theta_initial_irr

theta_new_irr = np.sum(theta_matrix_new_irr,axis =1)
theta_new_irr = np.sum(theta_new,axis = 0)
theta_new_irr

#likelihood 
like_new = np.sum(ln_pdf_new, axis = 1)
like_new = np.sum(like_new, axis = 0)

like_old = np.sum(ln_pdf,axis =1)
like_old = np.sum(like_old, axis = 0)

#likelihood irr
like_new_irr = np.sum(ln_pdf_irr_new, axis = 1)
like_new_irr = np.sum(like_new, axis = 0)

like_old_irr = np.sum(ln_pdf_irr,axis =1)
like_old_irr = np.sum(like_old, axis = 0)

prior_new = np.sum(log_prior_new,axis = 1)
prior_new = np.sum(prior_new,axis = 0)
prior_new = np.sum(prior_new,axis = 0)
prior_new

prior_old = np.sum(log_prior_old,axis = 1)
prior_old = np.sum(prior_old, axis = 0)
prior_old  = np.sum(prior_old,axis = 0)
prior_old

prior_new_irr = np.sum(log_prior_new_irr,axis = 1)
prior_new_irr = np.sum(prior_new_irr,axis = 0)
prior_new_irr = np.sum(prior_new_irr,axis = 0)
prior_new_irr

prior_old_irr = np.sum(log_prior_old_irr,axis = 1)
prior_old_irr = np.sum(prior_old_irr, axis = 0)
prior_old_irr  = np.sum(prior_old_irr,axis = 0)
prior_old_irr

def y_predicted(z,num_sample):

    dfObj = pd.DataFrame(z_matrix, columns=list("0123"))

    maxValueIndexObj = []

    # get the column name of max values in every row

    maxValueIndexObj.append(dfObj.idxmax(axis=1))

    y=[]



    maxValueIndexObj =asarray(maxValueIndexObj).reshape(num_sample, 1)

    y.append([int(i) for i in maxValueIndexObj])

    



    # print(maxValueIndexObj)

    # print("y is:", asarray(y))

    return asarray(y).T




y_predict = y_predicted(z_matrix,num_sample)
y_predict
results = confusion_matrix(y, y_predict)
print ('Confusion Matrix :')
print(results)
score=accuracy_score(y, y_predict)
print ('Accuracy Score :')
print(score)
print ('Report : ')
print(classification_report(y, y_predict))
accuracy = accuracy_score(y, y_predict)

print(a,b)


#print ('Confusion Matrix :')

#print(results) 

#score=accuracy_score(y, y_predict) 

#print ('Accuracy Score :')

#print(score)

#print ('Report : ')

#print(classification_report(y, y_predict))

#accuracy = accuracy_score(y, y_predict)

#print(a_,b_,c_)
#accuracy

def metropolis_hastings(theta_matrix_initial,theta_matrix_new,iterations):
  threshold = np.random.uniform(0, 1)
  accepted = []
  rejected = []

  x = theta_matrix_initial


  for i in range(iterations):
    x_new = theta_matrix_new
    r_nominator = theta_initial+(like_new_irr*prior_new)
    r_denominator = theta_new+(like_old_irr*prior_old)
    ratio = np.absolute(np.divide(r_nominator, r_denominator.T))
    r = np.exp(ratio)
    if r<threshold:
      x = x_new
      print(accepted.append(x_new))
    else:
      print(rejected.append(x_new))
        
      return np.array(accepted).shape, np.array(rejected)

mh = metropolis_hastings(theta_matrix_initial,theta_matrix_new,1000)
mh

def metropolis_hastings(theta_matrix_irr_initial,theta_matrix_new_irr,iterations):
  threshold = np.random.uniform(0, 1)
  accepted = []
  rejected = []

  x = theta_matrix_irr_initial


  for i in range(iterations):
    x_new_irr = theta_matrix_new_irr
    r_nominator = theta_initial_irr+(like_new_irr*prior_new_irr)
    r_denominator = theta_new+(like_old_irr*prior_old_irr)
    ratio = np.absolute(np.divide(r_nominator, r_denominator.T))
    r = np.exp(ratio)
    if r<threshold:
      x = x_new_irr
      print(accepted.append(x_new_irr))
    else:
      print(rejected.append(x_new_irr))
        
      return np.array(accepted).shape, np.array(rejected)

mh_irr = metropolis_hastings(theta_matrix_irr_initial,theta_matrix_new_irr,1000)
mh_irr

for i in range(-1,num_sample):
  for j in range(-1,num_cluster):
    for d in range(-1,num_dimension):
      part_one = x**(a[0,:]-1)
      part_one_one = x**(a[1,:]-1)
      part_one_two = x**(a[2,:]-1)
      part_one_three = x**(a[3,:]-1)
      part_1 = np.concatenate((part_one,part_one_one,part_one_two,part_one_three),axis = 0)
      part_1 = part_1.reshape(num_sample,num_cluster,num_dimension)
      
      one_data = np.subtract(1,x)
      part_two = one_data**(b[0,:]-1)
      part_two_one = one_data**(b[1,:]-1)
      part_two_two = one_data**(b[2,:]-1) 
      part_two_three = one_data**(b[3,:]-1)   
      part_2 = np.concatenate((part_two,part_two_one,part_two_two,part_two_three),axis = 0)
      part_2 = part_2.reshape(num_sample,num_cluster,num_dimension)


      part_three = one_data**(a[0,:]+b[0,:])
      part_three_one = one_data**(a[1,:]+b[1,:])
      part_three_two = one_data**(a[2,:]+b[2,:])
      part_three_three = one_data**(a[3,:]+b[3,:])
      part_3 = np.concatenate((part_three,part_three_one,part_three_two,part_three_three),axis = 0)
      part_3 = part_3.reshape(num_sample,num_cluster,num_dimension)

      whole = (part_1*part_2)/part_3
      
      whole_total = (rho)*whole

for j in range(-1,num_cluster):
    for d in range(-1,num_dimension):
      www = beta.pdf(x,a_irr[0,:],b_irr[0,:])
      ww1 = beta.pdf(x,a_irr[1,:],b_irr[1,:])
      ww2 = beta.pdf(x,a_irr[2,:],b_irr[2,:])
      ww3 = beta.pdf(x,a_irr[3,:],b_irr[3,:])
      part_w = np.concatenate((www,ww1,ww2,ww3),axis = 0)
      part_w = part_w.reshape(num_sample,num_cluster,num_dimension)

      part_w_total = (1-rho)*(part_w)

h = whole_total/(part_w_total*100)

for j in range(num_cluster):
  for k in range(num_dimension):
    h1 = np.where(h>1,1,0)
    hh = np.array([[np.count_nonzero(h1[:,j,k]==1) for k in range(num_dimension)] for j in range(num_cluster)])
hh

from scipy.stats import beta as beta_dist

def draw_Beta_dist(delta_a, delta_b):
    '''
    returns Bernoulli distributed samples
    '''
    return beta_dist.rvs(delta_a, delta_b)

# compute posterior for rho
t = np.ones([num_cluster,num_dimension])*5
zeta = np.ones([num_cluster,num_dimension])*5
rho1 = draw_Beta_dist(hh[0] + t[0], num_sample - hh[0] + zeta[0])
rho2 = draw_Beta_dist(hh[1] + t[1], num_sample - hh[1] + zeta[1])
rho3 = draw_Beta_dist(hh[2] + t[2], num_sample - hh[2] + zeta[2])
rho4 = draw_Beta_dist(hh[3] + t[3], num_sample - hh[3] + zeta[3])
rho_post = np.concatenate((rho1,rho2,rho3,rho4),axis = 0)
rho_post = part_w = rho_post.reshape(num_cluster,num_dimension)

rho3

rho_post